---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.16.1
  kernelspec:
    display_name: R
    name: ir
---

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 460}, id="OloGSAYfPVqg", outputId="a15b8c4b-656a-4c7c-f7c5-a36e3b5876f9"}
#importing the data
library(tidyverse)
library(dplyr)

df <- read.csv("healthcare-dataset-stroke-data.csv", na.strings = "N/A")
head(df)

```

<!-- #region id="U4gufkMDs5AQ" -->
# New Section
<!-- #endregion -->

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 34}, id="k24z5mtNReBI", outputId="c264d32b-e613-4484-bf88-6e702166fb36"}
#simple preprocessing
#removing id column
df <- df[, !(names(df) %in% "id")]
colnames(df)
```

```{python id="wXA1YSaERdLp"}
df$ever_married = factor(df$ever_married)
df$work_type = factor(df$work_type)
df$Residence_type = factor(df$Residence_type)
df$smoking_status = factor(df$smoking_status)
```

```{python id="P8dxJdBgRhFs"}

#added later
df$hypertension <- as.numeric(factor(df$hypertension))
df$heart_disease <- as.numeric(factor(df$heart_disease))

```

```{python id="XeOzJD3TRiqq"}
df <- df[df$gender != "Other", ]
df$gender = factor(df$gender)
```

```{python id="D8nXVoVQRq5v"}
median_bmi <- median(df$bmi, na.rm = TRUE)
df$bmi[is.na(df$bmi)] <- median_bmi
```

```{python colab={'base_uri': 'https://localhost:8080/'}, id="niDnJ3roRl04", outputId="58154ad6-0ab0-480f-835d-b24c1dd2f6e7"}
#removing row with other- only 1 row for that
str(df)
```

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 52}, id="ccwh7MiBRn9s", outputId="ac50f468-088b-4006-d267-270638062a56"}
na_values <- sum(is.na(df))
na_values_per_column <- colSums(is.na(df))
na_values_per_column
```

```{python colab={'base_uri': 'https://localhost:8080/'}, id="HXhoIdhkRu6p", outputId="9e459eeb-aa74-492f-e246-91b151d89839"}
install.packages('caret')


# # #min-max scaling of the numerical columns
# library(caret)
# # List of numeric columns you want to normalize
# numeric_columns <- c("age", "avg_glucose_level", "bmi")

# # Use preProcess to normalize
# preproc <- preProcess(df[, numeric_columns], method = c("range"))
# df_normalized <- predict(preproc, df[, numeric_columns])

# # Replace the original columns with the standardized data
# df[, numeric_columns] <- df_normalized

```

```{python colab={'base_uri': 'https://localhost:8080/'}, id="RwSgQ8h6RzJj", outputId="7d16afa7-f4af-4de9-d4ec-285f7ae5ffde"}
install.packages('ROSE')

library(ROSE)
library(caret)

#split the data into training and testing set
set.seed(25) # for reproducibility
sample_size <- floor(0.80 * nrow(df)) # 75% for training
train_indices <- sample(seq_len(nrow(df)), size = sample_size)

train_set <- df[train_indices, ]
test_set <- df[-train_indices, ]
```

```{python id="omCOwZu7R2Bw"}

train_set$stroke <- factor(train_set$stroke, levels = c(0, 1))
test_set$stroke <- factor(test_set$stroke, levels = c(0, 1))
```

```{python id="XpJzPeBXR4Hy"}
balanced_train_set <- ovun.sample(stroke ~ ., data = train_set, method = "over")$data
```

```{python colab={'base_uri': 'https://localhost:8080/'}, id="GfCL5McyPbTO", outputId="b410205f-35a4-430d-8fe7-f4b581cae8d1"}
install.packages('keras')
library(keras)
```

```{python id="R5pxa7XsbdM8"}
library(caret)
library(keras)
```

```{python colab={'base_uri': 'https://localhost:8080/'}, id="ry7De2tehnDN", outputId="029d8eb0-9a56-47d9-8d6e-25fa96dd8ece"}
str(balanced_train_set)
```

```{python id="cw9Zy8fPnWUg"}
library(caret)
library(dplyr)

X_train <- select(balanced_train_set, -stroke)  # Exclude the response variable
y_train <- balanced_train_set$stroke  # Directly access the column

# Convert y_train to numeric if it's a factor and not already numeric
y_train <- as.numeric(as.character(y_train))

# Apply dummyVars without fullRank to include all levels
dummyVarsOut <- dummyVars(~., data = X_train, fullRank = FALSE)
X_train <- predict(dummyVarsOut, X_train)

# Ensure X_train is a matrix, which is what Keras expects
X_train <- as.matrix(X_train)

preProcValues <- preProcess(X_train, method = c("center", "scale"))
X_train <- predict(preProcValues, X_train)

```

```{python id="HnnE9RXppDrk"}
if(any(sapply(X_train, is.character)) | any(sapply(y_train, is.character))) {
  stop("Error: String data detected in the training set. Please ensure all data is numeric.")
}
```

```{python colab={'base_uri': 'https://localhost:8080/'}, id="kpIeXS2XgSnT", outputId="042baf37-21a8-41d9-ae07-1b6fe4c42747"}
model <- keras_model_sequential() %>%
  layer_dense(units = 1024, activation = "relu", input_shape = ncol(X_train)) %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = 0.2) %>%

  layer_dense(units = 512, activation = "relu") %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = 0.2) %>%

  layer_dense(units = 256, activation = "relu") %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = 0.2) %>%

  layer_dense(units = 128, activation = "relu") %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = 0.2) %>%

  layer_dense(units = 64, activation = "relu") %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = 0.2) %>%

  layer_dense(units = 32, activation = "relu") %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = 0.2) %>%

  layer_dense(units = 1, activation = "sigmoid")  # For binary classification

# Compile the model
optimizer <- optimizer_rmsprop(learning_rate = 0.001)
model %>% compile(
  optimizer = optimizer,
  loss = 'binary_crossentropy',
  metrics = c('accuracy')
)


# Summary of the model
summary(model)

early_stopping <- callback_early_stopping(
  monitor = "val_loss",     # Monitor the validation loss
  patience = 25,            # Number of epochs with no improvement after which training will be stopped
  restore_best_weights = TRUE  # Restore model weights from the epoch with the best value of the monitored quantity
)

# Fit the model to the training data, might consider adjusting epochs and batch_size
history <- model %>% fit(
  X_train,
  y_train,
  epochs = 150, # Increased number of epochs
  batch_size = 64, # Adjusted batch size
  validation_split = 0.2,
  callbacks = list(early_stopping)
)
```

```{python id="EJg2wa25q3PC"}
library(caret)
library(dplyr)

# Assuming test_set_processed is your test dataset and has been defined similarly to balanced_train_set
X_test <- select(test_set, -stroke)  # Exclude the target variable
y_test <- test_set$stroke  # Access the target column

# Convert y_test to numeric if it's a factor and not already numeric
y_test <- as.numeric(as.character(y_test))

# Use the same dummyVars object (dummyVarsOut) created for X_train to transform X_test
# This ensures that X_test is processed with the exact same feature encoding as X_train
X_test <- predict(dummyVarsOut, newdata = X_test)

# Ensure X_test is a matrix, which is what Keras expects
X_test <- as.matrix(X_test)
X_test <- predict(preProcValues, X_test)
```

```{python colab={'base_uri': 'https://localhost:8080/'}, id="eId-S-0tWXo3", outputId="243de5e2-ed63-48d4-995a-f285291128bc"}
# Evaluate the model on the test data
evaluation_results <- model %>% evaluate(X_test, y_test, verbose = 0)

# Since the structure of evaluation_results might not be a named list, use indexing
# Normally, the first element is loss, and the second is accuracy, but this can depend on how you've compiled your model
cat("Test Loss:", evaluation_results[1], "\n")
cat("Test Accuracy:", evaluation_results[2], "\n")

```

```{python id="XPctedkL_WSO"}
library(keras)

# Assume 'model' is your trained Keras model
save_model_hdf5(model, "neural_networks.h5")

```
